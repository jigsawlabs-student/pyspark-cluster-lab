{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colonial-employment",
   "metadata": {},
   "source": [
    "# Pyspark Cluster Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-collect",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-meeting",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice connecting to a Pyspark cluster, and partitioning our dataset.  We'll do so by. working with a dataset of Netflix titles.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-divide",
   "metadata": {},
   "source": [
    "### Getting Setup (On Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-version",
   "metadata": {},
   "source": [
    "* Begin by installing some pip packages and the java development kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-creature",
   "metadata": {},
   "source": [
    "* Then set the java environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-sperm",
   "metadata": {},
   "source": [
    "* Then connect to a SparkSession, setting the spark ui port to `4050`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"netflix\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-breath",
   "metadata": {},
   "source": [
    "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excessive-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-allen",
   "metadata": {},
   "source": [
    "* And finally we get a link our Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-sacramento",
   "metadata": {},
   "source": [
    "### Viewing our Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-edition",
   "metadata": {},
   "source": [
    "Now get a link to an interface to view our cluster, simply by referencing our Spark Context, or we can view the link outputted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "honest-oasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>netflix</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=netflix>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-thunder",
   "metadata": {},
   "source": [
    "If we click on the blue `Spark UI` link above, this will take us to the Spark UI dashboard.  From there, the toolbar at the top and click on the `executors` tab.  From there, we'll see something like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-latex",
   "metadata": {},
   "source": [
    "<img src=\"./executors.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-round",
   "metadata": {},
   "source": [
    "So we can see that under `Cores`, it says 4.  And notice that our executor id says driver, which makes sense -- because when everything is local we do not have worker nodes that are separate from our driver node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-updating",
   "metadata": {},
   "source": [
    "### Reading some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-popularity",
   "metadata": {},
   "source": [
    "Ok, now let's load some data into our Spark cluster.  To do so, we can first read in our Netflix data using a separate library, pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wicked-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/jigsawlabs-student/pyspark-cluster-lab/main/netflix_titles.csv\"\n",
    "df = pd.read_csv(url)\n",
    "movies = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-hearts",
   "metadata": {},
   "source": [
    "So above, movies is a list of dictionaries in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "basic-bride",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'show_id': 's1',\n",
       "  'type': 'TV Show',\n",
       "  'title': '3%',\n",
       "  'director': nan,\n",
       "  'cast': 'João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi',\n",
       "  'country': 'Brazil',\n",
       "  'date_added': 'August 14, 2020',\n",
       "  'release_year': 2020,\n",
       "  'rating': 'TV-MA',\n",
       "  'duration': '4 Seasons',\n",
       "  'listed_in': 'International TV Shows, TV Dramas, TV Sci-Fi & Fantasy',\n",
       "  'description': 'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-romania",
   "metadata": {},
   "source": [
    "Ok, from here, let's distribute our data across the executors of our cluster.  We can do see by passing our data into the `parallelize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "motivated-dimension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd = None\n",
    "movies_rdd\n",
    "\n",
    "# ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-citation",
   "metadata": {},
   "source": [
    "And from there, let's see the number of partitions that our data is broken into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "endless-correction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-calvin",
   "metadata": {},
   "source": [
    "Now we'll learn more about how to use the Spark UI to see what Spark is doing, but perhaps one way is to look at the memory consumed. We can view this by again going to the executors tab, and then looking at the storage memory.   Notice that none of the memory was consumed, even though we directed Spark to create an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-transcript",
   "metadata": {},
   "source": [
    "> <img src=\"./executors_mem.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-stanford",
   "metadata": {},
   "source": [
    "### Performing Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-bangladesh",
   "metadata": {},
   "source": [
    "Ok, next use the `filter` and method to see all of movies from the country `'Brazil'`.  We'll call the `collect` method for you in the next line, so that you can see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "affiliated-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "brazil_movies_rdd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "floating-alberta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'show_id': 's1',\n",
       "  'type': 'TV Show',\n",
       "  'title': '3%',\n",
       "  'director': nan,\n",
       "  'cast': 'João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi',\n",
       "  'country': 'Brazil',\n",
       "  'date_added': 'August 14, 2020',\n",
       "  'release_year': 2020,\n",
       "  'rating': 'TV-MA',\n",
       "  'duration': '4 Seasons',\n",
       "  'listed_in': 'International TV Shows, TV Dramas, TV Sci-Fi & Fantasy',\n",
       "  'description': 'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brazil_movies_rdd.collect()[:1]\n",
    "\n",
    "# [{'show_id': 's1',\n",
    "#   'type': 'TV Show',\n",
    "#   'title': '3%',\n",
    "#   'director': nan,\n",
    "#   'cast': 'João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi',\n",
    "#   'country': 'Brazil',\n",
    "#   'date_added': 'August 14, 2020',\n",
    "#   'release_year': 2020,\n",
    "#   'rating': 'TV-MA',\n",
    "#   'duration': '4 Seasons',\n",
    "#   'listed_in': 'International TV Shows, TV Dramas, TV Sci-Fi & Fantasy',\n",
    "#   'description': 'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-lawsuit",
   "metadata": {},
   "source": [
    "So now if we go back to the executors tab, we can see that some of our memory at this point was consumed.  This tells us that once we saw this result, Spark used some of the memory to save the result.\n",
    "\n",
    "Ok, let's do another query.  This time, use spark to find the ratings of us movies.  \n",
    "> Once again, we'll call collect for you in the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "brilliant-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ratings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "talented-values",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PG-13', 'PG-13']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_ratings.collect()[:2]\n",
    "# ['PG-13', 'PG-13']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-clear",
   "metadata": {},
   "source": [
    "And again we'll take another look at the storage memory that's consumed.  This time we can see that we consumed additional memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-stone",
   "metadata": {},
   "source": [
    "> <img src='./filter_map_mem.png' width='60%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-amateur",
   "metadata": {},
   "source": [
    "> The important part is developing a sense for when Spark is storing our data in memory.  It seems like that when we perform queries, Spark is saving our results in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-director",
   "metadata": {},
   "source": [
    "Finally, it's a good idea to shutdown the context when we are done working with it.  We can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "checked-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-liberal",
   "metadata": {},
   "source": [
    "> Notice that if we now try to go to the Spark UI by say refreshing the page, we cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-brunei",
   "metadata": {},
   "source": [
    "<img src=\"./site_not_reach.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-ocean",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-wichita",
   "metadata": {},
   "source": [
    "In this lab, we practiced connecting to a cluster, creating a distributed dataset that we can query in parallel, and using the Spark UI to get a better understanding of the operations in our cluster.  By monitoring the memory consumption in our cluster, we saw that Spark is storing the results of our operations in memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
