{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ruled-innocent",
   "metadata": {},
   "source": [
    "# Pyspark Cluster Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-amateur",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-rider",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice connecting to a Pyspark cluster, and partitioning our dataset.  We'll do so by. working with a dataset of Netflix titles.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-station",
   "metadata": {},
   "source": [
    "### Connecting to our Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-production",
   "metadata": {},
   "source": [
    "Let's begin by connecting to our cluster.  To do so, we'll need to create a Spark context that connects to our cluster's driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "affected-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-secret",
   "metadata": {},
   "source": [
    "Begin by creating the configuration object with an app name of `netflix`, connect to the local computer, and have the cluster use 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "metropolitan-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"netflix\").setMaster(\"local[4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-serbia",
   "metadata": {},
   "source": [
    "Then create the Spark Context, and pass in the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "pressing-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-wrestling",
   "metadata": {},
   "source": [
    "### Viewing our Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "minimal-teaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jeffreys-air.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>netflix</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=netflix>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-hurricane",
   "metadata": {},
   "source": [
    "If we click on the Spark UI, and then click on the `executors` tab on the toolbar at the top, we can see something like the following under the Executors table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-listing",
   "metadata": {},
   "source": [
    "<img src=\"./executors.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-portland",
   "metadata": {},
   "source": [
    "So we can see that under `Cores`, it says 4.  And notice that our executor id says driver, which makes sense -- because when everything is local we do not have worker nodes that are separate from our driver node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-prisoner",
   "metadata": {},
   "source": [
    "### Reading some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-entrance",
   "metadata": {},
   "source": [
    "Ok, now let's create an RDD.  To do so, we can first read in our Netflix data using a separate library, pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "preliminary-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./netflix_titles.csv')\n",
    "movies = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "blessed-waste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'show_id': 's1',\n",
       "  'type': 'TV Show',\n",
       "  'title': '3%',\n",
       "  'director': nan,\n",
       "  'cast': 'João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi',\n",
       "  'country': 'Brazil',\n",
       "  'date_added': 'August 14, 2020',\n",
       "  'release_year': 2020,\n",
       "  'rating': 'TV-MA',\n",
       "  'duration': '4 Seasons',\n",
       "  'listed_in': 'International TV Shows, TV Dramas, TV Sci-Fi & Fantasy',\n",
       "  'description': 'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-stupid",
   "metadata": {},
   "source": [
    "Ok, from here, let's create our rdd, and can do so with the `parallelize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hazardous-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rdd = sc.parallelize(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-favor",
   "metadata": {},
   "source": [
    "And from there, let's see the number of partitions that our data is broken into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "corrected-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_rdd.getNumPartitions()\n",
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-economy",
   "metadata": {},
   "source": [
    "Now we'll learn more about how to use the Spark UI to see what Spark is doing, but perhaps one way is to look at the memory consumed. We can view this by again going to the executors tab, and then looking at the storage memory.   Notice that none of the memory was consumed, even though we directed Spark to create an RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-personality",
   "metadata": {},
   "source": [
    "<img src=\"./executors_mem.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-mandate",
   "metadata": {},
   "source": [
    "### Performing Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-campus",
   "metadata": {},
   "source": [
    "Ok use the `filter` and method to see all of movies from the country `'Brazil'`.  We'll call the collect method for you in the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "painted-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "brazil_movies_rdd = movies_rdd.filter(lambda movie: movie['country'] == 'Brazil')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-marker",
   "metadata": {},
   "source": [
    "Press shift + return on the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "joined-musical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'show_id': 's1',\n",
       "  'type': 'TV Show',\n",
       "  'title': '3%',\n",
       "  'director': nan,\n",
       "  'cast': 'João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi',\n",
       "  'country': 'Brazil',\n",
       "  'date_added': 'August 14, 2020',\n",
       "  'release_year': 2020,\n",
       "  'rating': 'TV-MA',\n",
       "  'duration': '4 Seasons',\n",
       "  'listed_in': 'International TV Shows, TV Dramas, TV Sci-Fi & Fantasy',\n",
       "  'description': 'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brazil_movies_rdd.collect()[:1]\n",
    "\n",
    "# [{'show_id': 's1',\n",
    "#   'type': 'TV Show',\n",
    "#   'title': '3%',\n",
    "#   'director': nan,\n",
    "#   'cast': 'João Miguel, Bianca Comparato, Michel Gomes, Rodolfo Valente, Vaneza Oliveira, Rafael Lozano, Viviane Porto, Mel Fronckowiak, Sergio Mamberti, Zezé Motta, Celso Frateschi',\n",
    "#   'country': 'Brazil',\n",
    "#   'date_added': 'August 14, 2020',\n",
    "#   'release_year': 2020,\n",
    "#   'rating': 'TV-MA',\n",
    "#   'duration': '4 Seasons',\n",
    "#   'listed_in': 'International TV Shows, TV Dramas, TV Sci-Fi & Fantasy',\n",
    "#   'description': 'In a future where the elite inhabit an island paradise far from the crowded slums, you get one chance to join the 3% saved from squalor.'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-present",
   "metadata": {},
   "source": [
    "So now if we go back to the executors tab, we can see that some of our memory at this point was consumed.  This tells us that once we saw this result, Spark used some of the memory to save the result.\n",
    "\n",
    "Ok, let's do another query.  This time, use spark to find the ratings of us movies.  \n",
    "> Once again, we'll call collect for you in the next line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "united-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ratings = movies_rdd.filter(lambda movie: movie['country'] == 'United States').map(lambda movie: movie['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acoustic-tribe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PG-13', 'PG-13']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_ratings.collect()[:2]\n",
    "# ['PG-13', 'PG-13']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-judges",
   "metadata": {},
   "source": [
    "And again we'll take another look at the storage memory that's consumed.  This time we can see that we consumed additional memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-necessity",
   "metadata": {},
   "source": [
    "<img src='./filter_map_mem.png' width='60%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-profession",
   "metadata": {},
   "source": [
    "> The important part is developing a sense for when Spark is persisting data.  It seems like that when we perform queries, Spark is saving our results in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-gates",
   "metadata": {},
   "source": [
    "Finally, it's a good idea to shutdown the context when we are done working with it.  We can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "analyzed-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-tomato",
   "metadata": {},
   "source": [
    "> Notice that if we now try to go to the Spark UI by say refreshing the page, we cannot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-organic",
   "metadata": {},
   "source": [
    "<img src=\"./site_not_reach.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-blast",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-vampire",
   "metadata": {},
   "source": [
    "In this lab, we practiced connecting to a cluster, creating our RDDs, and using the Spark UI to get a better understanding of the operations in our cluster.  By monitoring the memory consumption in our cluster, we saw that Spark is storing the results of our operations in memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
